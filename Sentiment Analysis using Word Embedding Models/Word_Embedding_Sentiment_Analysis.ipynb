{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from string import punctuation\n",
    "from collections import Counter , defaultdict\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from tabulate import tabulate\n",
    "import gensim\n",
    "import struct\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataX = data['review']\n",
    "DataY = data['sentiment']\n",
    "y = DataY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lancelot du lac ( lancelot du lac ) ( france ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>director : brian de palma writer : david koepp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>six days , seven nights reviewed by jamie peck...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cast : mel gibson ( jerry fletcher ) , julia r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>all great things come to an end , and the dot-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  lancelot du lac ( lancelot du lac ) ( france ,...          1\n",
       "1  director : brian de palma writer : david koepp...          1\n",
       "2  six days , seven nights reviewed by jamie peck...          1\n",
       "3  cast : mel gibson ( jerry fletcher ) , julia r...          1\n",
       "4  all great things come to an end , and the dot-...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def String_Splitter(data):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    for i in range(len(data)):\n",
    "        try :             \n",
    "            data[i]=data[i].split()\n",
    "        except:\n",
    "            print(i)\n",
    "            data[i] = data[i]\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def wordToString(wordList):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    stringList = []\n",
    "    for i in range(0,len(wordList)):\n",
    "        stringList.append(' '.join(word for word in wordList[i]))\n",
    "    return stringList\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def TextCleaner(Data, stopwordsList= ''):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    dataList = Data\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    CountVector = []\n",
    "    \n",
    "    for i in range(0, len(dataList)):\n",
    "        vocabulary = []\n",
    "        \n",
    "        for j in range(0, len(dataList[i])):\n",
    "            dataList[i][j] = stemmer.stem(dataList[i][j])\n",
    "            dataList[i][j] = lemmatizer.lemmatize(dataList[i][j])\n",
    "            \n",
    "        dataList[i] = [word for word in dataList[i] if word.isalpha()]\n",
    "        dataList[i] = [w.translate(table) for w in dataList[i]]\n",
    "        dataList[i] = [word for word in dataList[i] if len(word) > 3]\n",
    "        vocabulary.append(dataList[i])\n",
    "        \n",
    "    return dataList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_corpus = DataX.values.tolist()\n",
    "\n",
    "i = len(temp_corpus)-1\n",
    "while i != 0:\n",
    "    if temp_corpus[i]==float('nan'):\n",
    "        del(temp_corpus[i])\n",
    "    i-=1\n",
    "    \n",
    "\n",
    "new_data = String_Splitter(temp_corpus)\n",
    "X = TextCleaner(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['lancelot',\n",
       "  'lancelot',\n",
       "  'franc',\n",
       "  'film',\n",
       "  'review',\n",
       "  'jame',\n",
       "  'kendrick',\n",
       "  'director',\n",
       "  'robert',\n",
       "  'bresson',\n",
       "  'screenwrit',\n",
       "  'robert',\n",
       "  'bresson',\n",
       "  'base',\n",
       "  'epic',\n",
       "  'poem',\n",
       "  'chretien',\n",
       "  'troy',\n",
       "  'star',\n",
       "  'simon',\n",
       "  'lancelot',\n",
       "  'laura',\n",
       "  'duke',\n",
       "  'condomina',\n",
       "  'queen',\n",
       "  'guinever',\n",
       "  'vladimir',\n",
       "  'king',\n",
       "  'arthur',\n",
       "  'humbert',\n",
       "  'balsan',\n",
       "  'gawain',\n",
       "  'patrick',\n",
       "  'bernhard',\n",
       "  'modr',\n",
       "  'arthur',\n",
       "  'montalembert',\n",
       "  'lionel',\n",
       "  'mpaa',\n",
       "  'rate',\n",
       "  'rate',\n",
       "  'review',\n",
       "  'ench',\n",
       "  'director',\n",
       "  'robert',\n",
       "  'bresson',\n",
       "  'lancelot',\n",
       "  'lake',\n",
       "  'achiev',\n",
       "  'exact',\n",
       "  'opposit',\n",
       "  'effect',\n",
       "  'thoma',\n",
       "  'malori',\n",
       "  'mort',\n",
       "  'darthur',\n",
       "  'instead',\n",
       "  'enshrin',\n",
       "  'legend',\n",
       "  'king',\n",
       "  'arthur',\n",
       "  'knight',\n",
       "  'round',\n",
       "  'tabl',\n",
       "  'dethron',\n",
       "  'them',\n",
       "  'reveal',\n",
       "  'arthur',\n",
       "  'weak',\n",
       "  'ineffectu',\n",
       "  'leader',\n",
       "  'knight',\n",
       "  'group',\n",
       "  'jealous',\n",
       "  'bicker',\n",
       "  'fail',\n",
       "  'live',\n",
       "  'legend',\n",
       "  'prescrib',\n",
       "  'them',\n",
       "  'chivalri',\n",
       "  'place',\n",
       "  'lancelot',\n",
       "  'lake',\n",
       "  'except',\n",
       "  'that',\n",
       "  'ideal',\n",
       "  'bresson',\n",
       "  'begin',\n",
       "  'tale',\n",
       "  'knight',\n",
       "  'round',\n",
       "  'tabl',\n",
       "  'return',\n",
       "  'decim',\n",
       "  'after',\n",
       "  'fail',\n",
       "  'merlin',\n",
       "  'command',\n",
       "  'retriev',\n",
       "  'holi',\n",
       "  'grail',\n",
       "  'mystic',\n",
       "  'that',\n",
       "  'fill',\n",
       "  'with',\n",
       "  'christ',\n",
       "  'blood',\n",
       "  'bresson',\n",
       "  'immedi',\n",
       "  'give',\n",
       "  'impress',\n",
       "  'essenti',\n",
       "  'mean',\n",
       "  'grail',\n",
       "  'quest',\n",
       "  'bloodsh',\n",
       "  'failur',\n",
       "  'open',\n",
       "  'sequenc',\n",
       "  'seri',\n",
       "  'clumsi',\n",
       "  'disjoint',\n",
       "  'fight',\n",
       "  'amongst',\n",
       "  'anonym',\n",
       "  'knight',\n",
       "  'head',\n",
       "  'hack',\n",
       "  'stomach',\n",
       "  'impal',\n",
       "  'skull',\n",
       "  'split',\n",
       "  'open',\n",
       "  'skelet',\n",
       "  'remain',\n",
       "  'hang',\n",
       "  'from',\n",
       "  'tree',\n",
       "  'burn',\n",
       "  'bodi',\n",
       "  'smolder',\n",
       "  'ruin',\n",
       "  'flame',\n",
       "  'hous',\n",
       "  'sinc',\n",
       "  'film',\n",
       "  'start',\n",
       "  'with',\n",
       "  'camelot',\n",
       "  'take',\n",
       "  'onli',\n",
       "  'hour',\n",
       "  'half',\n",
       "  'arriv',\n",
       "  'inevit',\n",
       "  'conclus',\n",
       "  'carri',\n",
       "  'grand',\n",
       "  'tragic',\n",
       "  'reson',\n",
       "  'other',\n",
       "  'arthurian',\n",
       "  'film',\n",
       "  'never',\n",
       "  'camelot',\n",
       "  'peak',\n",
       "  'power',\n",
       "  'therefor',\n",
       "  'there',\n",
       "  'real',\n",
       "  'downfal',\n",
       "  'then',\n",
       "  'again',\n",
       "  'name',\n",
       "  'camelot',\n",
       "  'that',\n",
       "  'evok',\n",
       "  'titl',\n",
       "  'film',\n",
       "  'rather',\n",
       "  'lancelot',\n",
       "  'bresson',\n",
       "  'more',\n",
       "  'interest',\n",
       "  'intern',\n",
       "  'battl',\n",
       "  'within',\n",
       "  'heart',\n",
       "  'than',\n",
       "  'extern',\n",
       "  'downfal',\n",
       "  'kingdom',\n",
       "  'lancelot',\n",
       "  'alway',\n",
       "  'been',\n",
       "  'tragic',\n",
       "  'figur',\n",
       "  'arthurian',\n",
       "  'tale',\n",
       "  'bresson',\n",
       "  'central',\n",
       "  'figur',\n",
       "  'explor',\n",
       "  'battl',\n",
       "  'between',\n",
       "  'spirit',\n",
       "  'flesh',\n",
       "  'greatest',\n",
       "  'knight',\n",
       "  'flaw',\n",
       "  'onli',\n",
       "  'love',\n",
       "  'arthur',\n",
       "  'wife',\n",
       "  'queen',\n",
       "  'guinever',\n",
       "  'that',\n",
       "  'illicit',\n",
       "  'affair',\n",
       "  'that',\n",
       "  'eventu',\n",
       "  'caus',\n",
       "  'downfal',\n",
       "  'camelot',\n",
       "  'even',\n",
       "  'when',\n",
       "  'lancelot',\n",
       "  'attempt',\n",
       "  'affair',\n",
       "  'with',\n",
       "  'guinever',\n",
       "  'laura',\n",
       "  'duke',\n",
       "  'condomina',\n",
       "  'onli',\n",
       "  'find',\n",
       "  'himself',\n",
       "  'fall',\n",
       "  'back',\n",
       "  'into',\n",
       "  'against',\n",
       "  'better',\n",
       "  'judgment',\n",
       "  'know',\n",
       "  'mean',\n",
       "  'destruct',\n",
       "  'ideal',\n",
       "  'kingdom',\n",
       "  'powerless',\n",
       "  'passion',\n",
       "  'when',\n",
       "  'mordr',\n",
       "  'patrick',\n",
       "  'bernhard',\n",
       "  'accus',\n",
       "  'lancelot',\n",
       "  'affair',\n",
       "  'other',\n",
       "  'knight',\n",
       "  'includ',\n",
       "  'gawain',\n",
       "  'humbert',\n",
       "  'balsan',\n",
       "  'spring',\n",
       "  'lancelot',\n",
       "  'defens',\n",
       "  'this',\n",
       "  'battl',\n",
       "  'within',\n",
       "  'knight',\n",
       "  'that',\n",
       "  'eventu',\n",
       "  'undo',\n",
       "  'round',\n",
       "  'tabl',\n",
       "  'flesh',\n",
       "  'over',\n",
       "  'spirit',\n",
       "  'consequ',\n",
       "  'dire',\n",
       "  'bresson',\n",
       "  'intens',\n",
       "  'person',\n",
       "  'filmmak',\n",
       "  'most',\n",
       "  'interest',\n",
       "  'interior',\n",
       "  'heart',\n",
       "  'mind',\n",
       "  'lancelot',\n",
       "  'lake',\n",
       "  'fill',\n",
       "  'with',\n",
       "  'particular',\n",
       "  'trademark',\n",
       "  'minimalist',\n",
       "  'style',\n",
       "  'flat',\n",
       "  'expressionless',\n",
       "  'dialogu',\n",
       "  'grand',\n",
       "  'natur',\n",
       "  'sound',\n",
       "  'place',\n",
       "  'music',\n",
       "  'background',\n",
       "  'music',\n",
       "  'onli',\n",
       "  'twice',\n",
       "  'film',\n",
       "  'dure',\n",
       "  'open',\n",
       "  'narrat',\n",
       "  'segment',\n",
       "  'dure',\n",
       "  'open',\n",
       "  'credit',\n",
       "  'music',\n",
       "  'here',\n",
       "  'heavi',\n",
       "  'drumbeat',\n",
       "  'accompani',\n",
       "  'bagpip',\n",
       "  'rest',\n",
       "  'film',\n",
       "  'score',\n",
       "  'with',\n",
       "  'natur',\n",
       "  'sound',\n",
       "  'that',\n",
       "  'punctuat',\n",
       "  'film',\n",
       "  'themat',\n",
       "  'element',\n",
       "  'incess',\n",
       "  'clank',\n",
       "  'creak',\n",
       "  'heavi',\n",
       "  'armor',\n",
       "  'neigh',\n",
       "  'hors',\n",
       "  'rhythm',\n",
       "  'hoov',\n",
       "  'beat',\n",
       "  'down',\n",
       "  'dirt',\n",
       "  'road',\n",
       "  'natur',\n",
       "  'chirp',\n",
       "  'whisper',\n",
       "  'forest',\n",
       "  'like',\n",
       "  'most',\n",
       "  'other',\n",
       "  'film',\n",
       "  'bresson',\n",
       "  'employ',\n",
       "  'nonprofession',\n",
       "  'actor',\n",
       "  'recit',\n",
       "  'dialogu',\n",
       "  'emotionless',\n",
       "  'flat',\n",
       "  'voic',\n",
       "  'actor',\n",
       "  'lancelot',\n",
       "  'lake',\n",
       "  'never',\n",
       "  'befor',\n",
       "  'with',\n",
       "  'except',\n",
       "  'patrick',\n",
       "  'bernhard',\n",
       "  'they',\n",
       "  'never',\n",
       "  'again',\n",
       "  'never',\n",
       "  'they',\n",
       "  'rais',\n",
       "  'their',\n",
       "  'voic',\n",
       "  'emphasi',\n",
       "  'given',\n",
       "  'word',\n",
       "  'instead',\n",
       "  'vocal',\n",
       "  'inflect',\n",
       "  'bresson',\n",
       "  'strove',\n",
       "  'creat',\n",
       "  'emot',\n",
       "  'through',\n",
       "  'imag',\n",
       "  'some',\n",
       "  'this',\n",
       "  'techniqu',\n",
       "  'work',\n",
       "  'other',\n",
       "  'final',\n",
       "  'montag',\n",
       "  'arthur',\n",
       "  'battl',\n",
       "  'each',\n",
       "  'other',\n",
       "  'quit',\n",
       "  'marvel',\n",
       "  'final',\n",
       "  'imag',\n",
       "  'knight',\n",
       "  'shine',\n",
       "  'armor',\n",
       "  'reduc',\n",
       "  'liter',\n",
       "  'entir',\n",
       "  'film',\n",
       "  'moment',\n",
       "  'howev',\n",
       "  'other',\n",
       "  'time',\n",
       "  'bresson',\n",
       "  'uncompromis',\n",
       "  'method',\n",
       "  'distract',\n",
       "  'question',\n",
       "  'instanc',\n",
       "  'dure',\n",
       "  'import',\n",
       "  'joust',\n",
       "  'contest',\n",
       "  'bresson',\n",
       "  'film',\n",
       "  'major',\n",
       "  'action',\n",
       "  'that',\n",
       "  'onli',\n",
       "  'thing',\n",
       "  'visibl',\n",
       "  'hors',\n",
       "  'this',\n",
       "  'repeat',\n",
       "  'open',\n",
       "  'each',\n",
       "  'shot',\n",
       "  'with',\n",
       "  'same',\n",
       "  'note',\n",
       "  'from',\n",
       "  'bagpip',\n",
       "  'rais',\n",
       "  'differ',\n",
       "  'flag',\n",
       "  'while',\n",
       "  'there',\n",
       "  'might',\n",
       "  'symbol',\n",
       "  'valu',\n",
       "  'this',\n",
       "  'result',\n",
       "  'experi',\n",
       "  'watch',\n",
       "  'bothersom',\n",
       "  'nevertheless',\n",
       "  'lancelot',\n",
       "  'lake',\n",
       "  'fascin',\n",
       "  'cinemat',\n",
       "  'experi',\n",
       "  'bold',\n",
       "  'made',\n",
       "  'master',\n",
       "  'filmmak',\n",
       "  'bresson',\n",
       "  'style',\n",
       "  'everyon',\n",
       "  'respect',\n",
       "  'strength',\n",
       "  'artist',\n",
       "  'arthurian',\n",
       "  'legend',\n",
       "  'make',\n",
       "  'them',\n",
       "  'turn',\n",
       "  'lancelot',\n",
       "  'lake',\n",
       "  'into',\n",
       "  'someth',\n",
       "  'rare',\n",
       "  'modern',\n",
       "  'cinema',\n",
       "  'truli',\n",
       "  'person',\n",
       "  'film']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_corpora = \"\"\n",
    "for i in range(len(data)): \n",
    "    whole_corpora = whole_corpora + \" \" + data.iloc[i]['review'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_path = \"cc.en.300.vec\"\n",
    "encoding = \"utf-8\"\n",
    "with open(word_embedding_path, \"rb\") as lines:\n",
    "    wvec = {line.split()[0].decode(encoding): np.array(line.split()[1:],dtype=np.float32)\n",
    "               for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_small = {}\n",
    "all_words = set(w for words in X for w in words) # Making a dictionary of mutual words\n",
    "                                         # in the corpora and the word embedding model.\n",
    "with open(word_embedding_path, \"rb\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode(encoding)\n",
    "        if (word in all_words):\n",
    "            nums=np.array(parts[1:], dtype=np.float32)\n",
    "            glove_small[word] = nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mahdi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# train word2vec on all the texts - both training and test set\n",
    "# we're not using test labels, just texts so this is fine\n",
    "model = Word2Vec(X, size=100, window=5, min_count=5, workers=2)\n",
    "w2v = {w: vec for w, vec in zip(model.wv.index2word, model.wv.syn0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with the classics - naive bayes of the multinomial and bernoulli varieties\n",
    "# with either pure counts or tfidf features\n",
    "mult_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "mult_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "# SVM - which is supposed to be more or less state of the art \n",
    "# http://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf\n",
    "svc = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"linear svc\", SVC(kernel=\"linear\"))])\n",
    "svc_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"linear svc\", SVC(kernel=\"linear\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(glove_small))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "    \n",
    "# and a tf-idf version of the same\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(glove_small))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEV = MeanEmbeddingVectorizer(glove_small)\n",
    "MEV.fit(X,y)\n",
    "feature_matrix = MEV.transform(X)\n",
    "\n",
    "\n",
    "\n",
    "TFEV = TfidfEmbeddingVectorizer(glove_small)\n",
    "TFEV.fit(X,y)\n",
    "TF_feature_matrix = TFEV.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.31113054e-02, -3.05542577e-04, -1.23092420e-02,  6.84455112e-02,\n",
       "       -1.64194051e-02,  2.39376514e-03,  1.01905307e-02, -4.71686013e-03,\n",
       "       -7.15057459e-03, -4.93094837e-03, -2.76561007e-02, -2.07378808e-02,\n",
       "       -3.55912326e-03, -1.45734465e-02, -3.65196448e-03,  1.62067004e-02,\n",
       "        1.33644333e-02, -6.67690812e-03, -1.08526573e-02,  4.32840968e-03,\n",
       "        5.42240264e-03,  1.17348656e-02,  1.30667435e-02, -1.18037069e-03,\n",
       "       -1.82498787e-02, -2.13859156e-02, -1.15207853e-02, -6.32331765e-04,\n",
       "       -8.90530925e-03,  9.10438523e-02,  3.65196168e-03,  3.03879846e-03,\n",
       "       -1.02831442e-02, -2.09106337e-02, -3.04665091e-03,  4.89145471e-03,\n",
       "        2.43348535e-03,  2.51489710e-02, -5.20969648e-03,  1.51732238e-03,\n",
       "        5.13164047e-03, -7.08013913e-03, -1.27316425e-02, -1.08983507e-03,\n",
       "       -2.98064668e-02,  2.37358026e-02, -1.18117779e-02,  8.20023380e-03,\n",
       "       -1.66136343e-02,  4.21662582e-03,  1.26738949e-02, -4.63349186e-03,\n",
       "        1.93697549e-02, -3.98729183e-03, -1.08882301e-02,  1.02972332e-02,\n",
       "        2.34300159e-02, -1.52655033e-04, -4.52669784e-02, -8.44113703e-04,\n",
       "        2.34877504e-02,  1.16715925e-02,  8.15889146e-03, -6.29537692e-03,\n",
       "       -1.10762019e-03,  2.06976999e-02, -2.44743563e-02,  2.73066796e-02,\n",
       "       -2.28332579e-02, -6.98475912e-03, -4.37459489e-03,  1.51198562e-02,\n",
       "       -7.37067079e-03, -3.72447842e-03, -1.64618883e-02,  3.04503739e-03,\n",
       "        7.81986688e-04,  1.58300232e-02,  7.31039094e-03, -3.86697310e-03,\n",
       "       -2.31937692e-02,  2.56154761e-02,  1.40464269e-02,  1.30547341e-02,\n",
       "        3.02309450e-03, -1.61976963e-02, -3.27482657e-03, -6.05496764e-03,\n",
       "        1.34586561e-02,  1.08674373e-02, -4.83787525e-03,  1.97175555e-02,\n",
       "        2.60374080e-02,  1.39757460e-02,  1.02990773e-02,  1.25235599e-02,\n",
       "        1.15642082e-02,  2.02399548e-02,  6.11547497e-04,  9.23025049e-03,\n",
       "        3.44018498e-03, -2.81709037e-03,  1.18263178e-02, -6.38106093e-03,\n",
       "        2.84720529e-02,  8.18614475e-03,  7.26235751e-03, -5.49330330e-03,\n",
       "       -1.51133919e-02,  1.20530918e-03,  2.26540342e-02,  7.82493968e-03,\n",
       "       -1.27267921e-02,  3.18863876e-02,  7.31870392e-03, -1.42819835e-02,\n",
       "       -5.14018349e-03, -6.12170715e-03, -1.73634961e-02,  1.27337193e-02,\n",
       "       -1.05900681e-02, -2.89445766e-03, -2.45796749e-03, -6.14110846e-03,\n",
       "       -7.70900771e-03,  2.39085350e-02, -2.54873023e-03,  1.56873055e-02,\n",
       "       -3.74147780e-02,  1.35526592e-02,  8.87806341e-03, -4.12655994e-03,\n",
       "       -7.07390113e-03,  7.74364639e-03,  1.19789811e-02, -2.41861120e-02,\n",
       "        2.71293242e-03,  1.23311756e-02,  7.96765962e-04, -5.15842671e-03,\n",
       "       -1.93771403e-02,  1.65727418e-02, -2.12239596e-04, -8.18106066e-03,\n",
       "       -1.91713553e-02,  5.75912232e-03, -1.50497362e-01, -5.94272278e-03,\n",
       "       -5.71662514e-03, -6.53533684e-03, -1.78558938e-02, -7.33856717e-03,\n",
       "        2.21397225e-02, -2.69307382e-03, -6.86443597e-03, -7.84157589e-03,\n",
       "        5.00528775e-02, -7.13510392e-03,  2.00782884e-02,  1.14588896e-02,\n",
       "        7.82632921e-03,  1.51401889e-02,  2.75390353e-02, -3.10854632e-02,\n",
       "        1.31080858e-02, -5.63510414e-03, -4.63648979e-03,  8.78429599e-03,\n",
       "        6.94688363e-03,  5.58521599e-03, -2.77320854e-03, -7.08706724e-03,\n",
       "       -1.40094720e-02,  1.89692900e-02, -5.60738088e-04, -7.74594140e-04,\n",
       "       -8.10300838e-03,  2.43198872e-02, -9.60739236e-03, -7.71708786e-03,\n",
       "       -1.44494250e-02,  1.78245036e-03, -1.51291024e-02, -4.03708965e-02,\n",
       "        1.47757465e-02,  5.88105852e-03,  1.33016100e-02, -1.15277078e-02,\n",
       "        1.16916783e-02, -1.63464267e-02,  8.18499178e-03,  2.10515056e-02,\n",
       "        9.57736839e-03,  5.00230817e-04, -3.79214971e-03,  7.30762025e-03,\n",
       "       -9.00716335e-03, -5.76628372e-03,  1.00535788e-02,  3.55212502e-02,\n",
       "       -1.60909891e-02,  4.56932820e-02,  2.25399565e-02,  1.19265607e-02,\n",
       "       -1.59330165e-03, -2.26051104e-03, -7.34318933e-03,  3.35403951e-03,\n",
       "        1.26581937e-02,  1.27833802e-02, -9.75704752e-03, -6.34734659e-03,\n",
       "       -6.11801492e-03,  1.10009247e-02,  1.24757481e-03,  3.66812828e-03,\n",
       "        3.36697325e-03, -1.64288729e-02,  1.34242419e-02,  2.34572799e-03,\n",
       "       -1.03303161e-03, -4.47343802e-03, -4.19907412e-03,  2.77367122e-02,\n",
       "       -2.23789793e-02, -9.76027269e-03, -8.02077295e-04, -1.90808321e-03,\n",
       "        2.90831388e-03, -1.32295592e-02, -3.18649001e-02,  5.88960713e-03,\n",
       "        5.08776226e-04,  2.67769024e-02,  3.07648852e-02, -5.03810775e-03,\n",
       "        1.58226267e-02,  2.35325620e-02, -8.98106117e-03, -1.84574947e-02,\n",
       "        1.29607404e-02,  1.59055442e-02, -3.09357848e-02, -9.80667844e-02,\n",
       "        1.57999948e-01, -1.92886998e-03, -1.05080893e-02,  2.96027702e-03,\n",
       "       -7.29145249e-03,  7.18314433e-03, -1.21004619e-02, -1.06235282e-04,\n",
       "       -4.43233363e-03,  3.32104191e-02,  1.41154788e-02,  2.32124771e-03,\n",
       "       -4.86166030e-02, -3.87665033e-02,  9.19399597e-03, -1.28630456e-02,\n",
       "       -1.04563497e-02, -6.79330295e-03,  4.76027746e-03,  8.25911667e-03,\n",
       "       -2.23383382e-02,  3.57090030e-03,  1.51007082e-02,  9.51963826e-04,\n",
       "       -1.24953752e-02, -1.31337140e-02, -6.57644402e-03,  2.84341769e-03,\n",
       "       -1.89764332e-02, -2.02517305e-02, -8.26396700e-03, -1.20089207e-05,\n",
       "       -1.09413425e-02,  1.89757664e-02, -1.53083159e-02, -4.70484979e-03,\n",
       "        1.53741390e-02,  3.07598035e-03, -5.44935279e-02, -1.77397262e-02,\n",
       "       -5.18591283e-03,  1.89699829e-02,  3.35981557e-03, -1.12658255e-02,\n",
       "       -4.14226530e-03, -3.47660445e-02,  3.55708860e-02, -5.90046123e-03,\n",
       "       -8.44814926e-02,  1.85286365e-02,  2.07598251e-03,  1.05817523e-02,\n",
       "       -1.11004561e-02,  8.63353685e-02, -1.57327913e-02, -1.62341762e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.03867811e-02, -7.37054227e-03, -6.70127720e-02,  2.41524920e-01,\n",
       "       -9.57998913e-03,  1.39007065e-03,  3.56517285e-02, -2.35017464e-02,\n",
       "       -3.80988009e-02, -3.52753066e-02, -1.04767896e-01, -8.23728815e-02,\n",
       "       -2.10841354e-02, -6.51720986e-02, -2.59066727e-02,  7.23229647e-02,\n",
       "        5.51743098e-02, -1.11048175e-02, -3.30254994e-02,  6.69181021e-03,\n",
       "        4.40071523e-02,  3.52497138e-02,  3.52309458e-02, -4.80314065e-03,\n",
       "       -2.40648109e-02, -4.69535477e-02, -4.63568792e-02,  7.39399157e-03,\n",
       "       -2.81552095e-02,  3.96615952e-01, -1.66295224e-03,  1.65128242e-02,\n",
       "       -4.50002924e-02, -2.60777529e-02, -1.87330022e-02, -3.59837222e-03,\n",
       "        2.35757176e-02,  9.53708068e-02, -1.16779655e-02,  1.26356725e-02,\n",
       "        2.58193891e-02, -3.33762392e-02, -4.46071103e-02, -1.14148641e-02,\n",
       "       -1.27179563e-01,  7.67266154e-02, -3.43475789e-02,  3.31966467e-02,\n",
       "       -7.66736567e-02, -1.38697736e-02,  2.05815844e-02, -5.03384694e-02,\n",
       "        7.75212049e-02, -2.67778710e-02, -5.16902059e-02,  4.72674072e-02,\n",
       "        9.91417542e-02,  1.44199673e-02, -1.19687438e-01,  3.02575063e-02,\n",
       "        9.93664041e-02,  7.49492049e-02,  6.29361346e-02, -1.20276632e-02,\n",
       "        5.53508708e-03,  7.87776187e-02, -1.01761810e-01,  1.14495724e-01,\n",
       "       -1.00260615e-01, -2.87089795e-02,  1.58339012e-02,  5.73647693e-02,\n",
       "       -3.45064923e-02, -4.26888699e-03, -9.85113531e-02,  3.70039865e-02,\n",
       "       -1.18708266e-02,  4.19637896e-02,  1.38171157e-02, -1.10865701e-02,\n",
       "       -7.07724243e-02,  9.62768868e-02,  7.29185119e-02,  4.21869829e-02,\n",
       "       -7.98252132e-03, -6.77630305e-02, -7.43950997e-03, -2.95892954e-02,\n",
       "        5.43339700e-02,  2.10806285e-03, -2.48063616e-02,  6.81724474e-02,\n",
       "        8.10252875e-02,  8.42482671e-02,  3.99573334e-02,  1.76115166e-02,\n",
       "        4.83732410e-02,  9.48718786e-02,  8.08220264e-03,  3.65285613e-02,\n",
       "       -1.11961914e-02, -7.47732772e-03,  5.07581383e-02,  1.59342997e-02,\n",
       "        1.39144093e-01,  6.42777234e-02,  1.61693320e-02, -4.85641230e-03,\n",
       "       -3.99907306e-02, -1.73019506e-02,  7.17995912e-02,  5.40139563e-02,\n",
       "       -4.61270846e-02,  1.09763905e-01,  1.87398568e-02, -2.28854679e-02,\n",
       "        1.49759576e-02, -2.75380537e-02, -1.03281982e-01,  6.56892732e-02,\n",
       "       -4.63586599e-02, -1.31069385e-02, -8.66874773e-03, -4.65820506e-02,\n",
       "       -5.14170192e-02,  8.19553137e-02, -1.63723156e-02,  7.99713656e-02,\n",
       "       -1.62428349e-01,  1.61522496e-02,  8.38229954e-02, -5.78391925e-02,\n",
       "       -8.62771552e-03,  2.83935550e-03,  3.56815346e-02, -1.06855132e-01,\n",
       "        3.36275660e-02,  3.93992588e-02, -1.30719244e-02, -1.00649241e-02,\n",
       "       -6.99463859e-02,  7.76093230e-02,  5.47165936e-03, -4.08088528e-02,\n",
       "       -7.89495483e-02, -1.57603330e-03, -4.93422240e-01, -5.45587167e-02,\n",
       "       -8.57690442e-03, -9.70635656e-03, -5.09472899e-02, -5.07831275e-02,\n",
       "        6.71333224e-02, -3.88919488e-02, -3.25080976e-02, -5.34970239e-02,\n",
       "        1.28297791e-01, -5.76721914e-02,  6.71450496e-02,  4.31224070e-02,\n",
       "        3.23952399e-02,  4.42691296e-02,  1.05884202e-01, -1.23101115e-01,\n",
       "        5.54123111e-02, -4.19271141e-02, -2.28695478e-02,  2.29354482e-02,\n",
       "       -7.98735674e-03,  2.50473823e-02,  9.59900394e-03,  6.16719062e-03,\n",
       "       -9.04917642e-02,  1.18884258e-01,  1.55762322e-02, -3.09235044e-02,\n",
       "       -3.78538705e-02,  1.07303388e-01, -6.02919012e-02,  4.17483971e-03,\n",
       "       -5.53377271e-02,  9.03335493e-03, -6.09301217e-02, -1.66794211e-01,\n",
       "        5.48654720e-02,  8.44540168e-03,  7.36285448e-02, -3.48315872e-02,\n",
       "        3.59919332e-02, -6.06293678e-02,  6.43861592e-02,  7.88029581e-02,\n",
       "        5.14235198e-02, -3.34029384e-02, -1.14342291e-02,  1.91636179e-02,\n",
       "       -3.21123004e-02, -2.81622577e-02,  3.30624133e-02,  9.85303521e-02,\n",
       "       -3.89806852e-02,  1.48996770e-01,  5.76594882e-02,  5.11411838e-02,\n",
       "       -1.37240253e-03, -4.20213863e-02, -5.70672425e-03,  3.94894555e-02,\n",
       "        7.10001588e-02,  4.82426137e-02, -5.19834347e-02, -2.24002562e-02,\n",
       "       -2.18719821e-02,  4.06279117e-02, -1.03682932e-02,  1.95996817e-02,\n",
       "        1.34777313e-03, -7.50058368e-02,  3.90504226e-02,  6.41747331e-03,\n",
       "       -2.89627519e-02, -2.76443213e-02, -1.03859333e-02,  8.29630643e-02,\n",
       "       -9.98427048e-02, -3.04905232e-02, -1.50184212e-02,  7.94643816e-03,\n",
       "       -3.39221768e-03, -4.74950671e-02, -1.26880899e-01,  3.81907672e-02,\n",
       "        2.26291716e-02,  9.39599723e-02,  1.09234281e-01,  6.58721896e-03,\n",
       "        5.59860840e-02,  8.61914903e-02, -5.75912893e-02, -6.27739429e-02,\n",
       "        6.81897476e-02,  4.87343892e-02, -9.82710496e-02, -3.15866172e-01,\n",
       "        5.10703504e-01, -1.67845841e-02, -6.90018982e-02,  1.18556609e-02,\n",
       "       -2.06032004e-02,  3.68134156e-02, -4.82667759e-02,  3.44209111e-04,\n",
       "       -3.78958844e-02,  1.49086490e-01,  8.54089558e-02, -5.50410664e-03,\n",
       "       -1.74111918e-01, -1.55862093e-01,  2.50553675e-02, -8.10323358e-02,\n",
       "       -2.19217166e-02, -4.77969460e-02,  4.92473282e-02,  3.75053994e-02,\n",
       "       -6.09208867e-02,  1.07124932e-02,  8.43614340e-02,  7.45995808e-03,\n",
       "       -3.73845436e-02, -1.14021683e-02, -2.51438841e-02,  1.69874672e-02,\n",
       "       -6.67006895e-02, -4.92911972e-02, -4.90583852e-02,  1.72905903e-02,\n",
       "       -4.02338058e-02,  1.00551993e-01, -4.09152694e-02, -3.13662365e-02,\n",
       "        8.02254081e-02, -4.02490003e-03, -1.56370074e-01, -5.39800264e-02,\n",
       "       -3.19448207e-03,  8.56678560e-02, -5.93746454e-03,  1.00076627e-02,\n",
       "        3.99367660e-02, -1.60403654e-01,  1.26475886e-01, -4.29666527e-02,\n",
       "       -3.41037601e-01,  9.35580060e-02,  3.14094350e-02,  2.97153015e-02,\n",
       "       -5.43955453e-02,  3.03045005e-01, -4.92987409e-02, -8.74897018e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_feature_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Trees classifier is almost universally great, let's stack it with our embeddings\n",
    "etree_glove_small = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_small)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_glove_small_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_small)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "\n",
    "\n",
    "etree_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model                score\n",
      "-----------------  -------\n",
      "svc_tfidf           0.8297\n",
      "mult_nb_tfidf       0.8023\n",
      "mult_nb             0.7980\n",
      "svc                 0.7857\n",
      "bern_nb             0.7641\n",
      "bern_nb_tfidf       0.7641\n",
      "glove_small         0.7071\n",
      "glove_small_tfidf   0.6876\n",
      "w2v                 0.5808\n",
      "w2v_tfidf           0.5758\n"
     ]
    }
   ],
   "source": [
    "all_models = [\n",
    "    (\"mult_nb\", mult_nb),\n",
    "    (\"mult_nb_tfidf\", mult_nb_tfidf),\n",
    "    (\"bern_nb\", bern_nb),\n",
    "    (\"bern_nb_tfidf\", bern_nb_tfidf),\n",
    "    (\"svc\", svc),\n",
    "    (\"svc_tfidf\", svc_tfidf),\n",
    "    (\"w2v\", etree_w2v),\n",
    "    (\"w2v_tfidf\", etree_w2v_tfidf),\n",
    "    (\"glove_small\", etree_glove_small),\n",
    "    (\"glove_small_tfidf\", etree_glove_small_tfidf),\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "unsorted_scores = [(name, cross_val_score(model, X, y, cv=5).mean()) for name, model in all_models]\n",
    "scores = sorted(unsorted_scores, key=lambda x: -x[1])\n",
    "\n",
    "\n",
    "print (tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
