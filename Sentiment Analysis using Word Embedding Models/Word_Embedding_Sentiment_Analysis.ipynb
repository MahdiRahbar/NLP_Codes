{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from string import punctuation\n",
    "from collections import Counter , defaultdict\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import gensim\n",
    "import struct\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataX = data['review']\n",
    "DataY = data['sentiment']\n",
    "y = DataY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lancelot du lac ( lancelot du lac ) ( france ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>director : brian de palma writer : david koepp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>six days , seven nights reviewed by jamie peck...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cast : mel gibson ( jerry fletcher ) , julia r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>all great things come to an end , and the dot-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  lancelot du lac ( lancelot du lac ) ( france ,...          1\n",
       "1  director : brian de palma writer : david koepp...          1\n",
       "2  six days , seven nights reviewed by jamie peck...          1\n",
       "3  cast : mel gibson ( jerry fletcher ) , julia r...          1\n",
       "4  all great things come to an end , and the dot-...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def String_Splitter(data):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    for i in range(len(data)):\n",
    "        try :             \n",
    "            data[i]=data[i].split()\n",
    "        except:\n",
    "            print(i)\n",
    "            data[i] = data[i]\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def wordToString(wordList):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    stringList = []\n",
    "    for i in range(0,len(wordList)):\n",
    "        stringList.append(' '.join(word for word in wordList[i]))\n",
    "    return stringList\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def TextCleaner(Data, stopwordsList= ''):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    dataList = Data\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    CountVector = []\n",
    "    \n",
    "    for i in range(0, len(dataList)):\n",
    "        vocabulary = []\n",
    "        \n",
    "        for j in range(0, len(dataList[i])):\n",
    "            dataList[i][j] = stemmer.stem(dataList[i][j])\n",
    "            dataList[i][j] = lemmatizer.lemmatize(dataList[i][j])\n",
    "            \n",
    "        dataList[i] = [word for word in dataList[i] if word.isalpha()]\n",
    "        dataList[i] = [w.translate(table) for w in dataList[i]]\n",
    "        dataList[i] = [word for word in dataList[i] if len(word) > 3]\n",
    "        vocabulary.append(dataList[i])\n",
    "        \n",
    "    return dataList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_corpus = DataX.values.tolist()\n",
    "\n",
    "i = len(temp_corpus)-1\n",
    "while i != 0:\n",
    "    if temp_corpus[i]==float('nan'):\n",
    "        del(temp_corpus[i])\n",
    "    i-=1\n",
    "    \n",
    "\n",
    "new_data = String_Splitter(temp_corpus)\n",
    "X = TextCleaner(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['lancelot',\n",
       "  'lancelot',\n",
       "  'franc',\n",
       "  'film',\n",
       "  'review',\n",
       "  'jame',\n",
       "  'kendrick',\n",
       "  'director',\n",
       "  'robert',\n",
       "  'bresson',\n",
       "  'screenwrit',\n",
       "  'robert',\n",
       "  'bresson',\n",
       "  'base',\n",
       "  'epic',\n",
       "  'poem',\n",
       "  'chretien',\n",
       "  'troy',\n",
       "  'star',\n",
       "  'simon',\n",
       "  'lancelot',\n",
       "  'laura',\n",
       "  'duke',\n",
       "  'condomina',\n",
       "  'queen',\n",
       "  'guinever',\n",
       "  'vladimir',\n",
       "  'king',\n",
       "  'arthur',\n",
       "  'humbert',\n",
       "  'balsan',\n",
       "  'gawain',\n",
       "  'patrick',\n",
       "  'bernhard',\n",
       "  'modr',\n",
       "  'arthur',\n",
       "  'montalembert',\n",
       "  'lionel',\n",
       "  'mpaa',\n",
       "  'rate',\n",
       "  'rate',\n",
       "  'review',\n",
       "  'ench',\n",
       "  'director',\n",
       "  'robert',\n",
       "  'bresson',\n",
       "  'lancelot',\n",
       "  'lake',\n",
       "  'achiev',\n",
       "  'exact',\n",
       "  'opposit',\n",
       "  'effect',\n",
       "  'thoma',\n",
       "  'malori',\n",
       "  'mort',\n",
       "  'darthur',\n",
       "  'instead',\n",
       "  'enshrin',\n",
       "  'legend',\n",
       "  'king',\n",
       "  'arthur',\n",
       "  'knight',\n",
       "  'round',\n",
       "  'tabl',\n",
       "  'dethron',\n",
       "  'them',\n",
       "  'reveal',\n",
       "  'arthur',\n",
       "  'weak',\n",
       "  'ineffectu',\n",
       "  'leader',\n",
       "  'knight',\n",
       "  'group',\n",
       "  'jealous',\n",
       "  'bicker',\n",
       "  'fail',\n",
       "  'live',\n",
       "  'legend',\n",
       "  'prescrib',\n",
       "  'them',\n",
       "  'chivalri',\n",
       "  'place',\n",
       "  'lancelot',\n",
       "  'lake',\n",
       "  'except',\n",
       "  'that',\n",
       "  'ideal',\n",
       "  'bresson',\n",
       "  'begin',\n",
       "  'tale',\n",
       "  'knight',\n",
       "  'round',\n",
       "  'tabl',\n",
       "  'return',\n",
       "  'decim',\n",
       "  'after',\n",
       "  'fail',\n",
       "  'merlin',\n",
       "  'command',\n",
       "  'retriev',\n",
       "  'holi',\n",
       "  'grail',\n",
       "  'mystic',\n",
       "  'that',\n",
       "  'fill',\n",
       "  'with',\n",
       "  'christ',\n",
       "  'blood',\n",
       "  'bresson',\n",
       "  'immedi',\n",
       "  'give',\n",
       "  'impress',\n",
       "  'essenti',\n",
       "  'mean',\n",
       "  'grail',\n",
       "  'quest',\n",
       "  'bloodsh',\n",
       "  'failur',\n",
       "  'open',\n",
       "  'sequenc',\n",
       "  'seri',\n",
       "  'clumsi',\n",
       "  'disjoint',\n",
       "  'fight',\n",
       "  'amongst',\n",
       "  'anonym',\n",
       "  'knight',\n",
       "  'head',\n",
       "  'hack',\n",
       "  'stomach',\n",
       "  'impal',\n",
       "  'skull',\n",
       "  'split',\n",
       "  'open',\n",
       "  'skelet',\n",
       "  'remain',\n",
       "  'hang',\n",
       "  'from',\n",
       "  'tree',\n",
       "  'burn',\n",
       "  'bodi',\n",
       "  'smolder',\n",
       "  'ruin',\n",
       "  'flame',\n",
       "  'hous',\n",
       "  'sinc',\n",
       "  'film',\n",
       "  'start',\n",
       "  'with',\n",
       "  'camelot',\n",
       "  'take',\n",
       "  'onli',\n",
       "  'hour',\n",
       "  'half',\n",
       "  'arriv',\n",
       "  'inevit',\n",
       "  'conclus',\n",
       "  'carri',\n",
       "  'grand',\n",
       "  'tragic',\n",
       "  'reson',\n",
       "  'other',\n",
       "  'arthurian',\n",
       "  'film',\n",
       "  'never',\n",
       "  'camelot',\n",
       "  'peak',\n",
       "  'power',\n",
       "  'therefor',\n",
       "  'there',\n",
       "  'real',\n",
       "  'downfal',\n",
       "  'then',\n",
       "  'again',\n",
       "  'name',\n",
       "  'camelot',\n",
       "  'that',\n",
       "  'evok',\n",
       "  'titl',\n",
       "  'film',\n",
       "  'rather',\n",
       "  'lancelot',\n",
       "  'bresson',\n",
       "  'more',\n",
       "  'interest',\n",
       "  'intern',\n",
       "  'battl',\n",
       "  'within',\n",
       "  'heart',\n",
       "  'than',\n",
       "  'extern',\n",
       "  'downfal',\n",
       "  'kingdom',\n",
       "  'lancelot',\n",
       "  'alway',\n",
       "  'been',\n",
       "  'tragic',\n",
       "  'figur',\n",
       "  'arthurian',\n",
       "  'tale',\n",
       "  'bresson',\n",
       "  'central',\n",
       "  'figur',\n",
       "  'explor',\n",
       "  'battl',\n",
       "  'between',\n",
       "  'spirit',\n",
       "  'flesh',\n",
       "  'greatest',\n",
       "  'knight',\n",
       "  'flaw',\n",
       "  'onli',\n",
       "  'love',\n",
       "  'arthur',\n",
       "  'wife',\n",
       "  'queen',\n",
       "  'guinever',\n",
       "  'that',\n",
       "  'illicit',\n",
       "  'affair',\n",
       "  'that',\n",
       "  'eventu',\n",
       "  'caus',\n",
       "  'downfal',\n",
       "  'camelot',\n",
       "  'even',\n",
       "  'when',\n",
       "  'lancelot',\n",
       "  'attempt',\n",
       "  'affair',\n",
       "  'with',\n",
       "  'guinever',\n",
       "  'laura',\n",
       "  'duke',\n",
       "  'condomina',\n",
       "  'onli',\n",
       "  'find',\n",
       "  'himself',\n",
       "  'fall',\n",
       "  'back',\n",
       "  'into',\n",
       "  'against',\n",
       "  'better',\n",
       "  'judgment',\n",
       "  'know',\n",
       "  'mean',\n",
       "  'destruct',\n",
       "  'ideal',\n",
       "  'kingdom',\n",
       "  'powerless',\n",
       "  'passion',\n",
       "  'when',\n",
       "  'mordr',\n",
       "  'patrick',\n",
       "  'bernhard',\n",
       "  'accus',\n",
       "  'lancelot',\n",
       "  'affair',\n",
       "  'other',\n",
       "  'knight',\n",
       "  'includ',\n",
       "  'gawain',\n",
       "  'humbert',\n",
       "  'balsan',\n",
       "  'spring',\n",
       "  'lancelot',\n",
       "  'defens',\n",
       "  'this',\n",
       "  'battl',\n",
       "  'within',\n",
       "  'knight',\n",
       "  'that',\n",
       "  'eventu',\n",
       "  'undo',\n",
       "  'round',\n",
       "  'tabl',\n",
       "  'flesh',\n",
       "  'over',\n",
       "  'spirit',\n",
       "  'consequ',\n",
       "  'dire',\n",
       "  'bresson',\n",
       "  'intens',\n",
       "  'person',\n",
       "  'filmmak',\n",
       "  'most',\n",
       "  'interest',\n",
       "  'interior',\n",
       "  'heart',\n",
       "  'mind',\n",
       "  'lancelot',\n",
       "  'lake',\n",
       "  'fill',\n",
       "  'with',\n",
       "  'particular',\n",
       "  'trademark',\n",
       "  'minimalist',\n",
       "  'style',\n",
       "  'flat',\n",
       "  'expressionless',\n",
       "  'dialogu',\n",
       "  'grand',\n",
       "  'natur',\n",
       "  'sound',\n",
       "  'place',\n",
       "  'music',\n",
       "  'background',\n",
       "  'music',\n",
       "  'onli',\n",
       "  'twice',\n",
       "  'film',\n",
       "  'dure',\n",
       "  'open',\n",
       "  'narrat',\n",
       "  'segment',\n",
       "  'dure',\n",
       "  'open',\n",
       "  'credit',\n",
       "  'music',\n",
       "  'here',\n",
       "  'heavi',\n",
       "  'drumbeat',\n",
       "  'accompani',\n",
       "  'bagpip',\n",
       "  'rest',\n",
       "  'film',\n",
       "  'score',\n",
       "  'with',\n",
       "  'natur',\n",
       "  'sound',\n",
       "  'that',\n",
       "  'punctuat',\n",
       "  'film',\n",
       "  'themat',\n",
       "  'element',\n",
       "  'incess',\n",
       "  'clank',\n",
       "  'creak',\n",
       "  'heavi',\n",
       "  'armor',\n",
       "  'neigh',\n",
       "  'hors',\n",
       "  'rhythm',\n",
       "  'hoov',\n",
       "  'beat',\n",
       "  'down',\n",
       "  'dirt',\n",
       "  'road',\n",
       "  'natur',\n",
       "  'chirp',\n",
       "  'whisper',\n",
       "  'forest',\n",
       "  'like',\n",
       "  'most',\n",
       "  'other',\n",
       "  'film',\n",
       "  'bresson',\n",
       "  'employ',\n",
       "  'nonprofession',\n",
       "  'actor',\n",
       "  'recit',\n",
       "  'dialogu',\n",
       "  'emotionless',\n",
       "  'flat',\n",
       "  'voic',\n",
       "  'actor',\n",
       "  'lancelot',\n",
       "  'lake',\n",
       "  'never',\n",
       "  'befor',\n",
       "  'with',\n",
       "  'except',\n",
       "  'patrick',\n",
       "  'bernhard',\n",
       "  'they',\n",
       "  'never',\n",
       "  'again',\n",
       "  'never',\n",
       "  'they',\n",
       "  'rais',\n",
       "  'their',\n",
       "  'voic',\n",
       "  'emphasi',\n",
       "  'given',\n",
       "  'word',\n",
       "  'instead',\n",
       "  'vocal',\n",
       "  'inflect',\n",
       "  'bresson',\n",
       "  'strove',\n",
       "  'creat',\n",
       "  'emot',\n",
       "  'through',\n",
       "  'imag',\n",
       "  'some',\n",
       "  'this',\n",
       "  'techniqu',\n",
       "  'work',\n",
       "  'other',\n",
       "  'final',\n",
       "  'montag',\n",
       "  'arthur',\n",
       "  'battl',\n",
       "  'each',\n",
       "  'other',\n",
       "  'quit',\n",
       "  'marvel',\n",
       "  'final',\n",
       "  'imag',\n",
       "  'knight',\n",
       "  'shine',\n",
       "  'armor',\n",
       "  'reduc',\n",
       "  'liter',\n",
       "  'entir',\n",
       "  'film',\n",
       "  'moment',\n",
       "  'howev',\n",
       "  'other',\n",
       "  'time',\n",
       "  'bresson',\n",
       "  'uncompromis',\n",
       "  'method',\n",
       "  'distract',\n",
       "  'question',\n",
       "  'instanc',\n",
       "  'dure',\n",
       "  'import',\n",
       "  'joust',\n",
       "  'contest',\n",
       "  'bresson',\n",
       "  'film',\n",
       "  'major',\n",
       "  'action',\n",
       "  'that',\n",
       "  'onli',\n",
       "  'thing',\n",
       "  'visibl',\n",
       "  'hors',\n",
       "  'this',\n",
       "  'repeat',\n",
       "  'open',\n",
       "  'each',\n",
       "  'shot',\n",
       "  'with',\n",
       "  'same',\n",
       "  'note',\n",
       "  'from',\n",
       "  'bagpip',\n",
       "  'rais',\n",
       "  'differ',\n",
       "  'flag',\n",
       "  'while',\n",
       "  'there',\n",
       "  'might',\n",
       "  'symbol',\n",
       "  'valu',\n",
       "  'this',\n",
       "  'result',\n",
       "  'experi',\n",
       "  'watch',\n",
       "  'bothersom',\n",
       "  'nevertheless',\n",
       "  'lancelot',\n",
       "  'lake',\n",
       "  'fascin',\n",
       "  'cinemat',\n",
       "  'experi',\n",
       "  'bold',\n",
       "  'made',\n",
       "  'master',\n",
       "  'filmmak',\n",
       "  'bresson',\n",
       "  'style',\n",
       "  'everyon',\n",
       "  'respect',\n",
       "  'strength',\n",
       "  'artist',\n",
       "  'arthurian',\n",
       "  'legend',\n",
       "  'make',\n",
       "  'them',\n",
       "  'turn',\n",
       "  'lancelot',\n",
       "  'lake',\n",
       "  'into',\n",
       "  'someth',\n",
       "  'rare',\n",
       "  'modern',\n",
       "  'cinema',\n",
       "  'truli',\n",
       "  'person',\n",
       "  'film']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_corpora = \"\"\n",
    "for i in range(len(data)): \n",
    "    whole_corpora = whole_corpora + \" \" + data.iloc[i]['review'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_path = \"cc.en.300.vec\"\n",
    "encoding = \"utf-8\"\n",
    "with open(word_embedding_path, \"rb\") as lines:\n",
    "    wvec = {line.split()[0].decode(encoding): np.array(line.split()[1:],dtype=np.float32)\n",
    "               for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_small = {}\n",
    "all_words = set(w for words in X for w in words) # Making a dictionary of mutual words\n",
    "                                         # in the corpora and the word embedding model.\n",
    "with open(word_embedding_path, \"rb\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode(encoding)\n",
    "        if (word in all_words):\n",
    "            nums=np.array(parts[1:], dtype=np.float32)\n",
    "            glove_small[word] = nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mahdi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# train word2vec on all the texts - both training and test set\n",
    "# we're not using test labels, just texts so this is fine\n",
    "model = Word2Vec(X, size=100, window=5, min_count=5, workers=2)\n",
    "w2v = {w: vec for w, vec in zip(model.wv.index2word, model.wv.syn0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with the classics - naive bayes of the multinomial and bernoulli varieties\n",
    "# with either pure counts or tfidf features\n",
    "mult_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "mult_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "# SVM - which is supposed to be more or less state of the art \n",
    "# http://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf\n",
    "svc = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"linear svc\", SVC(kernel=\"linear\"))])\n",
    "svc_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"linear svc\", SVC(kernel=\"linear\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(glove_small))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "    \n",
    "# and a tf-idf version of the same\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(glove_small))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEV = MeanEmbeddingVectorizer(glove_small)\n",
    "MEV.fit(X,y)\n",
    "feature_matrix = MEV.transform(X)\n",
    "\n",
    "\n",
    "\n",
    "TFEV = TfidfEmbeddingVectorizer(glove_small)\n",
    "TFEV.fit(X,y)\n",
    "TF_feature_matrix = TFEV.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.31113054e-02, -3.05542577e-04, -1.23092420e-02,  6.84455112e-02,\n",
       "       -1.64194051e-02,  2.39376514e-03,  1.01905307e-02, -4.71686013e-03,\n",
       "       -7.15057459e-03, -4.93094837e-03, -2.76561007e-02, -2.07378808e-02,\n",
       "       -3.55912326e-03, -1.45734465e-02, -3.65196448e-03,  1.62067004e-02,\n",
       "        1.33644333e-02, -6.67690812e-03, -1.08526573e-02,  4.32840968e-03,\n",
       "        5.42240264e-03,  1.17348656e-02,  1.30667435e-02, -1.18037069e-03,\n",
       "       -1.82498787e-02, -2.13859156e-02, -1.15207853e-02, -6.32331765e-04,\n",
       "       -8.90530925e-03,  9.10438523e-02,  3.65196168e-03,  3.03879846e-03,\n",
       "       -1.02831442e-02, -2.09106337e-02, -3.04665091e-03,  4.89145471e-03,\n",
       "        2.43348535e-03,  2.51489710e-02, -5.20969648e-03,  1.51732238e-03,\n",
       "        5.13164047e-03, -7.08013913e-03, -1.27316425e-02, -1.08983507e-03,\n",
       "       -2.98064668e-02,  2.37358026e-02, -1.18117779e-02,  8.20023380e-03,\n",
       "       -1.66136343e-02,  4.21662582e-03,  1.26738949e-02, -4.63349186e-03,\n",
       "        1.93697549e-02, -3.98729183e-03, -1.08882301e-02,  1.02972332e-02,\n",
       "        2.34300159e-02, -1.52655033e-04, -4.52669784e-02, -8.44113703e-04,\n",
       "        2.34877504e-02,  1.16715925e-02,  8.15889146e-03, -6.29537692e-03,\n",
       "       -1.10762019e-03,  2.06976999e-02, -2.44743563e-02,  2.73066796e-02,\n",
       "       -2.28332579e-02, -6.98475912e-03, -4.37459489e-03,  1.51198562e-02,\n",
       "       -7.37067079e-03, -3.72447842e-03, -1.64618883e-02,  3.04503739e-03,\n",
       "        7.81986688e-04,  1.58300232e-02,  7.31039094e-03, -3.86697310e-03,\n",
       "       -2.31937692e-02,  2.56154761e-02,  1.40464269e-02,  1.30547341e-02,\n",
       "        3.02309450e-03, -1.61976963e-02, -3.27482657e-03, -6.05496764e-03,\n",
       "        1.34586561e-02,  1.08674373e-02, -4.83787525e-03,  1.97175555e-02,\n",
       "        2.60374080e-02,  1.39757460e-02,  1.02990773e-02,  1.25235599e-02,\n",
       "        1.15642082e-02,  2.02399548e-02,  6.11547497e-04,  9.23025049e-03,\n",
       "        3.44018498e-03, -2.81709037e-03,  1.18263178e-02, -6.38106093e-03,\n",
       "        2.84720529e-02,  8.18614475e-03,  7.26235751e-03, -5.49330330e-03,\n",
       "       -1.51133919e-02,  1.20530918e-03,  2.26540342e-02,  7.82493968e-03,\n",
       "       -1.27267921e-02,  3.18863876e-02,  7.31870392e-03, -1.42819835e-02,\n",
       "       -5.14018349e-03, -6.12170715e-03, -1.73634961e-02,  1.27337193e-02,\n",
       "       -1.05900681e-02, -2.89445766e-03, -2.45796749e-03, -6.14110846e-03,\n",
       "       -7.70900771e-03,  2.39085350e-02, -2.54873023e-03,  1.56873055e-02,\n",
       "       -3.74147780e-02,  1.35526592e-02,  8.87806341e-03, -4.12655994e-03,\n",
       "       -7.07390113e-03,  7.74364639e-03,  1.19789811e-02, -2.41861120e-02,\n",
       "        2.71293242e-03,  1.23311756e-02,  7.96765962e-04, -5.15842671e-03,\n",
       "       -1.93771403e-02,  1.65727418e-02, -2.12239596e-04, -8.18106066e-03,\n",
       "       -1.91713553e-02,  5.75912232e-03, -1.50497362e-01, -5.94272278e-03,\n",
       "       -5.71662514e-03, -6.53533684e-03, -1.78558938e-02, -7.33856717e-03,\n",
       "        2.21397225e-02, -2.69307382e-03, -6.86443597e-03, -7.84157589e-03,\n",
       "        5.00528775e-02, -7.13510392e-03,  2.00782884e-02,  1.14588896e-02,\n",
       "        7.82632921e-03,  1.51401889e-02,  2.75390353e-02, -3.10854632e-02,\n",
       "        1.31080858e-02, -5.63510414e-03, -4.63648979e-03,  8.78429599e-03,\n",
       "        6.94688363e-03,  5.58521599e-03, -2.77320854e-03, -7.08706724e-03,\n",
       "       -1.40094720e-02,  1.89692900e-02, -5.60738088e-04, -7.74594140e-04,\n",
       "       -8.10300838e-03,  2.43198872e-02, -9.60739236e-03, -7.71708786e-03,\n",
       "       -1.44494250e-02,  1.78245036e-03, -1.51291024e-02, -4.03708965e-02,\n",
       "        1.47757465e-02,  5.88105852e-03,  1.33016100e-02, -1.15277078e-02,\n",
       "        1.16916783e-02, -1.63464267e-02,  8.18499178e-03,  2.10515056e-02,\n",
       "        9.57736839e-03,  5.00230817e-04, -3.79214971e-03,  7.30762025e-03,\n",
       "       -9.00716335e-03, -5.76628372e-03,  1.00535788e-02,  3.55212502e-02,\n",
       "       -1.60909891e-02,  4.56932820e-02,  2.25399565e-02,  1.19265607e-02,\n",
       "       -1.59330165e-03, -2.26051104e-03, -7.34318933e-03,  3.35403951e-03,\n",
       "        1.26581937e-02,  1.27833802e-02, -9.75704752e-03, -6.34734659e-03,\n",
       "       -6.11801492e-03,  1.10009247e-02,  1.24757481e-03,  3.66812828e-03,\n",
       "        3.36697325e-03, -1.64288729e-02,  1.34242419e-02,  2.34572799e-03,\n",
       "       -1.03303161e-03, -4.47343802e-03, -4.19907412e-03,  2.77367122e-02,\n",
       "       -2.23789793e-02, -9.76027269e-03, -8.02077295e-04, -1.90808321e-03,\n",
       "        2.90831388e-03, -1.32295592e-02, -3.18649001e-02,  5.88960713e-03,\n",
       "        5.08776226e-04,  2.67769024e-02,  3.07648852e-02, -5.03810775e-03,\n",
       "        1.58226267e-02,  2.35325620e-02, -8.98106117e-03, -1.84574947e-02,\n",
       "        1.29607404e-02,  1.59055442e-02, -3.09357848e-02, -9.80667844e-02,\n",
       "        1.57999948e-01, -1.92886998e-03, -1.05080893e-02,  2.96027702e-03,\n",
       "       -7.29145249e-03,  7.18314433e-03, -1.21004619e-02, -1.06235282e-04,\n",
       "       -4.43233363e-03,  3.32104191e-02,  1.41154788e-02,  2.32124771e-03,\n",
       "       -4.86166030e-02, -3.87665033e-02,  9.19399597e-03, -1.28630456e-02,\n",
       "       -1.04563497e-02, -6.79330295e-03,  4.76027746e-03,  8.25911667e-03,\n",
       "       -2.23383382e-02,  3.57090030e-03,  1.51007082e-02,  9.51963826e-04,\n",
       "       -1.24953752e-02, -1.31337140e-02, -6.57644402e-03,  2.84341769e-03,\n",
       "       -1.89764332e-02, -2.02517305e-02, -8.26396700e-03, -1.20089207e-05,\n",
       "       -1.09413425e-02,  1.89757664e-02, -1.53083159e-02, -4.70484979e-03,\n",
       "        1.53741390e-02,  3.07598035e-03, -5.44935279e-02, -1.77397262e-02,\n",
       "       -5.18591283e-03,  1.89699829e-02,  3.35981557e-03, -1.12658255e-02,\n",
       "       -4.14226530e-03, -3.47660445e-02,  3.55708860e-02, -5.90046123e-03,\n",
       "       -8.44814926e-02,  1.85286365e-02,  2.07598251e-03,  1.05817523e-02,\n",
       "       -1.11004561e-02,  8.63353685e-02, -1.57327913e-02, -1.62341762e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
